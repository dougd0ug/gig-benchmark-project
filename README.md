# ğŸ¯ GIG-Benchmark

**GIG-Benchmark** is a comprehensive sports betting odds comparison platform that automatically scrapes, processes, and displays betting odds from multiple bookmakers across various sports leagues.

The platform uses a distributed microservices architecture with automated web scraping, message queue processing, and a modern web interface to provide real-time odds comparison and arbitrage opportunity detection.

You can see our landing page for visual presentation : https://dougd0ug.github.io/gig-benchmark-project/

---

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [Architecture](#-architecture)
- [Prerequisites](#-prerequisites)
- [Installation](#-installation)
- [Usage](#-usage)
- [Project Structure](#-project-structure)
- [Services Overview](#-services-overview)
- [Data Flow](#-data-flow)
- [Development](#-development)
- [Supported Leagues](#-supported-leagues)
- [Contributing](#-contributing)

---

## âœ¨ Features

- **Automated Web Scraping**: Selenium-based scraping workers for multiple betting sites
- **Multi-League Support**: Football leagues including Ligue 1, Premier League, Bundesliga, A-League, and more
- **Real-Time Processing**: RabbitMQ message queue for asynchronous task processing
- **Arbitrage Detection**: Automatic calculation of Total Return on Investment (TRJ/ROI)
- **REST API**: Django-based API for data access and management
- **Modern Frontend**: Symfony-based web interface with dynamic odds display
- **Microservices Architecture**: 9 Docker services working together seamlessly
- **Scalable Design**: Message-driven architecture for horizontal scaling

---

## ğŸ—ï¸ Architecture

The platform consists of **9 Docker services** orchestrated with Docker Compose:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     GIG-BENCHMARK PLATFORM                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Nginx   â”‚  â”‚   PHP    â”‚  â”‚ Backend  â”‚  â”‚  MySQL   â”‚     â”‚
â”‚  â”‚  :10014  â”‚â—„â”€â”¤ Symfony  â”‚â—„â”€â”¤  Django  â”‚â—„â”€â”¤  :3307   â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  :8000   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                               â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                    â”‚                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Selenium â”‚â—„â”€â”¤ Scraping â”‚â—„â”€â”¤RabbitMQ â”‚â—„â”€â”¤ Consumer â”‚      â”‚
â”‚  â”‚  :4444   â”‚  â”‚  Worker  â”‚  â”‚ :5672   â”‚  â”‚   Odds   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                    â–²                        â”‚
â”‚                               â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                  â”‚
â”‚                               â”‚  Celery  â”‚                  â”‚
â”‚                               â”‚ Worker + â”‚                  â”‚
â”‚                               â”‚   Beat   â”‚                  â”‚
â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Prerequisites

Before installing the project, ensure you have:

- **Docker** >= 20.10
- **Docker Compose** >= 2.0
- **Git**
- At least **4GB of RAM** available for containers
- **Ports available**: 3307, 4444, 5672, 8000, 10014, 15672

---

## ğŸš€ Installation

### 1. Clone the repository

```bash
git clone https://github.com/yourusername/gig-benchmark.git
cd gig-benchmark
```

### 2. Configure environment variables

Create a `.env` file at the project root, ask us for the content.

### 3. Build and start all services

```bash
# Build all Docker images
docker compose build

# Start all services in detached mode
docker compose up -d

# Wait for services to initialize (about 30 seconds)
sleep 30

# Verify all services are running
docker compose ps
```

### 4. Initialize the database

Django migrations are automatically run on backend startup, but you can verify:

```bash
# Check migration status
docker compose exec backend python manage.py showmigrations

# Create a superuser for Django admin (optional)
docker compose exec backend python manage.py createsuperuser
```

### 5. Access the platform

- **Frontend**: http://localhost:10014
- **Django Admin**: http://localhost:8000/admin
- **RabbitMQ Management**: http://localhost:15672 (admin/admin)
- **Selenium VNC**: http://localhost:7900 (for debugging)

---

## ğŸ® Usage

### Triggering a Scraping Task

You can trigger scraping from the frontend interface or manually via command line:

```bash
# Method 1: Using the scraping service directly
docker compose exec scraping python send_task.py football.ligue_1

# Method 2: Check available scrapers
docker compose exec scraping python -c "from registry import SCRAPERS; print(list(SCRAPERS.keys()))"
```

### Monitoring the Process

```bash
# View scraping worker logs
docker compose logs scraping -f

# View consumer odds logs
docker compose logs consumer_odds -f

# View RabbitMQ queue status
docker compose exec rabbitmq rabbitmqctl list_queues

# View backend logs
docker compose logs backend -f
```

### Checking Results

```bash
# Access MySQL database
docker compose exec db mysql -u gig_user -p gig_benchmark

# Check stored matches
mysql> SELECT * FROM core_match LIMIT 10;

# Check stored odds
mysql> SELECT * FROM core_odd LIMIT 10;
```

---

## ğŸ“‚ Project Structure

```
gig-benchmark/
â”œâ”€â”€ backend/                    # Django REST API
â”‚   â”œâ”€â”€ config/                 # Django configuration
â”‚   â”œâ”€â”€ core/                   # Main application (models, views, serializers)
â”‚   â”œâ”€â”€ consumers/              # RabbitMQ consumers
â”‚   â”‚   â””â”€â”€ consumer_odds.py    # Odds consumer (reads 'odds' queue)
â”‚   â”œâ”€â”€ manage.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ frontend/                   # Symfony web interface
â”‚   â”œâ”€â”€ src/                    # PHP controllers and services
â”‚   â”œâ”€â”€ templates/              # Twig templates
â”‚   â”œâ”€â”€ public/                 # Public assets
â”‚   â”‚   â”œâ”€â”€ js/                 # JavaScript files
â”‚   â”‚   â”‚   â”œâ”€â”€ sidebar.js      # Scraping triggers and filters
â”‚   â”‚   â”‚   â”œâ”€â”€ login.js        # Authentication
â”‚   â”‚   â”‚   â”œâ”€â”€ navbar-auth.js  # Navigation
â”‚   â”‚   â”‚   â””â”€â”€ odds-loader.js  # Dynamic odds loading
â”‚   â”‚   â””â”€â”€ css/                # Stylesheets
â”‚   â”œâ”€â”€ composer.json
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ scraping/                   # Web scraping workers
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ football/           # Football league scrapers
â”‚   â”‚   â”‚   â”œâ”€â”€ ligue_1.py      # French Ligue 1
â”‚   â”‚   â”‚   â”œâ”€â”€ premier_league.py
â”‚   â”‚   â”‚   â”œâ”€â”€ bundesliga.py
â”‚   â”‚   â”‚   â”œâ”€â”€ a_league.py     # Australian A-League
â”‚   â”‚   â”‚   â””â”€â”€ _scraper_utils.py # Shared utilities
â”‚   â”‚   â””â”€â”€ registry.py         # Scraper registry
â”‚   â”œâ”€â”€ worker.py               # Main worker (listens to 'scraping_tasks')
â”‚   â”œâ”€â”€ send_task.py            # Task sender utility
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ database/                   # Database initialization
â”‚   â””â”€â”€ schema.sql              # (Deprecated - using Django migrations)
â”‚
â”œâ”€â”€ docker-compose.yml          # Docker services orchestration
â”œâ”€â”€ nginx.conf                  # Nginx configuration
â”œâ”€â”€ .env                        # Environment variables
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md                   # This file
```

---

## ğŸ³ Services Overview

### 1. **MySQL Database** (`db`)
- **Image**: `mysql:8.0`
- **Port**: `3307:3306`
- **Role**: Stores all application data (matches, odds, bookmakers, users)
- **Volume**: Persistent storage via `db_data` volume

### 2. **RabbitMQ** (`rabbitmq`)
- **Image**: `rabbitmq:3.12-management-alpine`
- **Ports**: `5672` (AMQP), `15672` (Management UI)
- **Role**: Message broker for asynchronous task processing
- **Queues**:
  - `scraping_tasks`: Receives scraping requests
  - `odds`: Receives scraped odds data

### 3. **Django Backend** (`backend`)
- **Port**: `8000`
- **Role**: REST API, admin interface, database management
- **Tech**: Django + Gunicorn (4 workers)
- **Endpoints**:
  - `/admin/` - Django admin
  - `/api/` - REST API endpoints

### 4. **Celery Worker** (`celery_worker`)
- **Role**: Processes asynchronous background tasks
- **Concurrency**: 4 workers
- **Use cases**: Scheduled tasks, batch processing

### 5. **Celery Beat** (`celery_beat`)
- **Role**: Task scheduler (cron-like)
- **Use cases**: Periodic scraping, data cleanup, maintenance tasks

### 6. **Consumer Odds** (`consumer_odds`)
- **Role**: Consumes messages from `odds` queue and stores them in MySQL
- **File**: `backend/consumers/consumer_odds.py`
- **Process**:
  1. Listens to `odds` queue
  2. Parses odds data
  3. Creates/updates Match, Odd, and Bookmaker records

### 7. **Selenium** (`selenium`)
- **Image**: `selenium/standalone-chrome`
- **Port**: `4444` (WebDriver), `7900` (VNC)
- **Role**: Headless Chrome browser for web scraping
- **Memory**: 3GB shared memory
- **Config**: Max 1 session, 5-minute timeout

### 8. **Scraping Worker** (`scraping`)
- **Role**: Web scraping orchestrator
- **Process**:
  1. Listens to `scraping_tasks` queue
  2. Loads appropriate scraper from registry
  3. Connects to Selenium for browser automation
  4. Scrapes betting sites (e.g., coteur.com)
  5. Sends results to `odds` queue

### 9. **Nginx + PHP-FPM** (`nginx` + `php`)
- **Port**: `10014`
- **Role**: Serves Symfony frontend application
- **Tech**: Nginx as reverse proxy + PHP 8.3-FPM

---

## ğŸ”„ Data Flow

Here's how a complete scraping cycle works:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. TRIGGER                                                 â”‚
â”‚    User clicks "Scrape" â†’ Frontend sends request           â”‚
â”‚    OR: python send_task.py football.ligue_1                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. RABBITMQ - Queue "scraping_tasks"                       â”‚
â”‚    Message: {"scraper": "football.ligue_1"}                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. SCRAPING WORKER                                         â”‚
â”‚    - Consumes message from "scraping_tasks"                â”‚
â”‚    - Loads scraper: scraping/src/football/ligue_1.py       â”‚
â”‚    - Connects to Selenium (port 4444)                      â”‚
â”‚    - Opens headless Chrome                                 â”‚
â”‚    - Navigates to betting site                             â”‚
â”‚    - Extracts match data and odds                          â”‚
â”‚    - For each match/bookmaker:                             â”‚
â”‚      â†’ Publishes message to "odds" queue                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. RABBITMQ - Queue "odds"                                 â”‚
â”‚    Multiple messages: {match, bookmaker, odds, trj}        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. CONSUMER ODDS                                           â”‚
â”‚    - Consumes messages from "odds" queue                   â”‚
â”‚    - Parses JSON data                                      â”‚
â”‚    - Creates/updates database records:                     â”‚
â”‚      â€¢ Match (team names, date, league)                    â”‚
â”‚      â€¢ Bookmaker (name, URL)                               â”‚
â”‚      â€¢ Odd (1, N, 2, TRJ)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. MYSQL DATABASE                                          â”‚
â”‚    Data stored and ready for display                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. FRONTEND DISPLAY                                        â”‚
â”‚    - User visits http://localhost:10014                    â”‚
â”‚    - Frontend queries Django API                           â”‚
â”‚    - Odds displayed with TRJ calculation                   â”‚
â”‚    - Arbitrage opportunities highlighted                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Development

### Running Individual Services

```bash
# Start only the database
docker compose up -d db

# Start backend and dependencies
docker compose up -d backend

# Restart a specific service
docker compose restart scraping

# Stop all services
docker compose down

# Stop and remove volumes (âš ï¸ deletes all data)
docker compose down -v
```

### Viewing Logs

```bash
# Follow logs for all services
docker compose logs -f

# Follow logs for specific service
docker compose logs -f scraping

# Show last 100 lines
docker compose logs --tail=100 backend
```

### Accessing Service Shells

```bash
# Django shell
docker compose exec backend python manage.py shell

# MySQL shell
docker compose exec db mysql -u gig_user -p

# Scraping worker bash
docker compose exec scraping bash

# PHP container bash
docker compose exec php bash
```

### Adding a New Scraper

1. Create a new scraper file in `scraping/src/football/`:

```python
# scraping/src/football/new_league.py
from ._scraper_utils import publish_odds, setup_driver
import pika

def scrape_new_league():
    """Scrape odds for New League"""
    return scrape_league(
        league_name="New League",
        league_url="https://www.coteur.com/NewLeague",
        display_name="NewLeague"
    )
```

2. Register it in `scraping/src/worker.py`:

```python
SCRAPERS = {
    'football.new_league': 'src.football.new_league.scrape_new_league',
    # ... other scrapers
}
```

3. Update frontend in `frontend/public/js/sidebar.js` to add the league button.

---

## âš½ Supported Leagues

Currently supported football leagues:

- **Ligue 1** (France) - `football.ligue_1`
- **Premier League** (England) - `football.premier_league`
- **Bundesliga** (Germany) - `football.bundesliga`
- **A-League** (Australia) - `football.a_league`
- **Serie A** (Italy) - `football.serie_a`
- **La Liga** (Spain) - `football.la_liga`

And others more.
---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these guidelines:

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/new-feature`
3. Make your changes
4. Ensure all services still work: `docker compose up`
5. Commit your changes: `git commit -m "Add new feature"`
6. Push to the branch: `git push origin feature/new-feature`
7. Create a Pull Request

---

## ğŸ› Known Issues

- Scraping may fail if betting sites change their HTML structure
- Large scraping jobs may require increasing Selenium memory limit

---

## ğŸ“§ Contact

For questions or support, please open an issue on GitHub.

---

**Built with â¤ï¸ using Django, Symfony, Selenium, RabbitMQ, and Docker**

Dorine LemÃ©e, Simon Paulin and Louis and Manchon.
